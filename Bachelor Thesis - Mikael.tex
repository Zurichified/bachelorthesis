\documentclass{seal_thesis}

\thesisType{Bachelor Thesis}
\date{\today}
\title{Effects of source code properties on variability of software microbenchmarks written in Go}

\author{Mikael Basmaci}
\home{Istanbul} % Geburtsort
\country{Turkey}
\legi{15-721-244}
\prof{Prof. Dr. Harald C. Gall}
\assistent{Christoph Laaber}
\email{mikael.basmaci@uzh.ch}
\url{<url if available>}
\begindate{25.03.2019}
\enddate{25.09.2019}

\begin{document}
\maketitle

\frontmatter

\begin{acknowledgements}
	Here comes the acknowledgements.
\end{acknowledgements}

\begin{abstract}
	Here comes the abstract.
\end{abstract}

\begin{zusammenfassung}
	Here comes the summary.
\end{zusammenfassung}

\tableofcontents
\listoffigures
\listoftables
\lstlistoflistings

\mainmatter

\chapter{Introduction}

Creating bug-free and stable software was one of the main goals of software engineering for many years. Stakeholders of a software system could probably be easily satisfied if the system did what it promised to do. Although this does not imply that performance was not an important aspect of software, it was still not the number one priority of the time.\\
\\
With the evolution of software engineering over the years, and the constantly advancing technology that makes software engineering possible, the importance of performance has grown. Having a performant system nowadays is not only the wish of customers or stakeholders, but also one of the goals of software developers when developing software. A better performance is important for many reasons. To give some examples: With better performance, a data driven application  will load the data faster, a calculation will result quicker, or an interactive software will be more responsive. All these examples show that more or same amount of work can be done in less time, with better performance.\\
\\
The usual way of testing software against bugs has been usage of test suites that include unit tests, which test specific parts of the software to assess their completeness. These are supported with integration and end-to-end tests, which go into a deeper level. For the performance testing of a software, there used to be no specific way of doing until the occurrence of performance testing frameworks, which have been getting more and more popular in the recent years. With early adoption of these frameworks in the development phase of a project, performance regressions can be detected and taken care of pre release.\\
\\
Rest of this thesis is structured as follows: I give an introduction to related topics about this thesis in the rest of the \textbf{Introduction} section. Next, I present some of the works that has been done before about benchmark variability in the \textbf{Related Work} section. In the \textbf{Methodology} section, I explain the three steps I take to do the analytic part of this thesis. I present the results of these steps in the \textbf{Results} section. Next comes the \textbf{Discussion} section where I discuss about the methodology and the results of this thesis. In the \textbf{Threats to the Validity} section I present some of the possible threats to this thesis and conclude with the \textbf{Conclusion} section.

\section{Software microbenchmarks}
- Different kinds of benchmarks found on literature \\
- What are software microbenchmarks \\
- Why are they used \\
- How are they used \\
\\
Different kinds of benchmarks are found in the literature. While sometimes benchmarks refer to a standardization of a measure, other benchmarks are here to assess the performance of the underlying system. For example, there are different benchmark programs to measure the performance of hardware such as Central Processing Unit(CPU), to be able to compare their performance with others of its kind. Another example can be the benchmark function of a game, which gives the average FPS score of the game run on a hardware.\\
\\
Software developers can analyze the performance of parts of their software with the so called software microbenchmarks, which can be defined as unit tests for performance of a software. These benchmarks, unlike other benchmarks that test a whole system for the performance, test only a small fraction of the software, such as one or multiple little functions.
Executing microbenchmarks for a defined time period, one can sample an average execution
time of the benchmark, as well as the data points as the results of multiple iterations.


\section{Benchmark variability}
- Explaining data points as the result of iterations \\
- Defining term "benchmark variability" \\
- Factors that play a role in the variability of benchmarks\\
- Why is it important to find out / predict the variability causes \\
\\
The variability of these results may depend on a lot of factors such as the execution platform,
the hardware the benchmarks are executed on, or even the programming language of the
software itself. It is important to predict the variability of these results to select the stable
benchmarks to be run, because executing performance tests of a software is usually a long,
time consuming process. Moreover, by predicting the variability, the stability of benchmarks can
be estimated. By understanding the root causes for benchmark variability and being able to predict these, we could support developers to write better benchmarks. One of the ways to predict the variability might be through analyzing source code properties of the software. This can on one hand help the developers identify the cause of slowdowns in a newer version for example, on the other hand help them understand which source code property affects the
results in which way.\\
\\
The goal of this thesis is to study the variability of results of performance tests on a large scale with many projects written in Go. For this, the reason for the high/low distribution of benchmark results of these Go Projects will be investigated based on their source code properties such as File IO, HTTP Calls, Loops, User IO and Memory Management, which can affect the performance rigorously.


\section{Golang}
- Some information about the programming language \\
- How is a benchmark defined in Go language, with examples and how can we execute these? \\
- Data points acquired from a previous study from Laaber et al. \\


\chapter{Related Work}
Here comes the literature research about benchmark variability. While some papers look at the benchmark variability causes, some of them try to predict the performance regressions based on different versions of a software by analyzing different commits and mining source code. \\
\\
To the best of my knowledge, there exist no study analyzing benchmark variability of software microbechmarks written in Go by trying to find a correlation with the used source code metrics in these benchmarks.

\chapter{Methodology}
In this section, I present the methodologies I followed to get to the results. As the project consists of 3 main steps, these steps are explained respectively.
\section{Calculating variabilities}
First part of the study involved doing a qualitative analysis by analyzing the given data set. From this data set, I firstly had to extract the valid projects, i. e. choose the projects that have a positive average benchmark score. Secondly, I calculated some metrics for each of benchmark that indicate the variability. The results follow in the first point of Results section.
\subsection{Data set}
The starting point was a data set that was acquired by Laaber et. al from a previous study? (Ask for details) \\
\\
Out of 482 projects, 230 of the projects qualified because they had a score bigger than -1 in the main file of data set. ...
\\
 This includes the categorization of benchmarks of 228 projects written in Go by looking at the total executions, the mean, coefficient of variation and relative confidence interval width with 95\% and 99\% confidence intervals. Secondly, we introduce the methodology we used to extract source code properties from in the first place chosen benchmarks. Thirdly, comparing the source code properties with the variabilities of benchmarks by using regression models to achieve the results.
 
\subsection{Metrics as the variability indicators}
For calculating the variabilities of benchmarks, I oriented myself at 3 main metrics. First metric is "Coefficient of Variation" (CV), which is a metric used often in statistics to calculate variability of a data set. ...

\subsection{Python coding}
How I coded the calculations and created the visualizations. \\
Which library I used for creating the visualization? \\
How I used the pa-tool to get the RCIW95 and RCIW99 values \\

\section{Extracting source code properties}
Second part of the study was to extract the source code properties of benchmarks written in Go. This also had 2 parts. Results of this part are to be found in second part of the Results section.

\subsection{Decision on source code properties}
Which steps I took at deciding for the source code properties

\subsection{Golang coding}
- How I coded in Go to extract the properties? (Also get into detail about AST's in Go and giving code examples) \\
- Which libraries or other tools I used (Callgraph tool, cyclomatic complexity tool) ? \\

\section{Finding correlations}
Third and last part of the study was to find correlations between source code properties of benchmarks and their variability. In this last part, I used a regression model on some chosen benchmarks to find correlations. Results of this part are to be found in the third part of Results section.
\subsection{First analysis - Second analysis}
- Which dependent and independent variables do I have for the comparison?
\subsection{Regression model}
- Which regression model I used for the results, how did I get the correlations results?

\chapter{Results}
In this section, I present the results from the 3 parts of this study. 
\section{Variabilities of benchmarks}
After calculating the variability metrics of benchmarks from the projects, I categorized the variabilities in 10 percent buckets starting from 0 o 100 percent. All the benchmarks that have a variability higher than 100 percent fall into one final bucket. ...

\section{Source code properties}
For every project that I was able to analyze by using the tool introduced in second part of methodology section, I was able to collect the source code properties. CSV? ...

\section{Correlation between variabilities and source code properties}
- Yes \\
- No \\
- Not really? \\
- Can't really say \\



\chapter{Discussion}
In this section, I discuss the results that I obtained and how relevant these are.
\section{Chosen properties}
Do the chosen properties make sense? \\
Why did I choose these properties and how could these affect the benchmark variability? \\

\section{Static analysis}
This study involved doing a static analysis for the source code of project written in Go. What pros and cons does this have? Why doing a dynamic analysis would make more sense?

\section{Size of data set}
Coming from 482 open source projects written in Go, down to 228 that I could analyze. Is the size of data set small or big enough to acknowledge the results?

\chapter{Threats to the validity}
There are threats to the validity of this study.

\section{Number of projects}
There are probably thousands of projects written with Go. These can be with or without benchmarks, however, I was limited to a number of projects for the outcome of this study and the number might not reflect the truth.

\section{Chosen properties}
I presented my chosen properties for this analysis, however, are these enough for this study? There may be other metrics that might be very relevant to this study, which I haven't discovered whilst searching.

\section{Analysis of the unknown}
Since the analysis is made statically for the second part of the methodology, this can be a threat to the validity since we don't actually know which nodes in the cyclomatic complexity are visited.

\chapter{Conclusion}
I finally conclude with what I have done with this project: How I started, which steps I took and which results I achieved.


\backmatter
\bibliographystyle{alpha}
\bibliography{}

\end{document}
